{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOAoJsAf9y00Dzpy68DpIc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guilhermelaviola/BIArchitectureAndBigData/blob/main/Class10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Analytical SCM**\n",
        "Studying the application of data analysis in supply chains to optimize efficiency, reduce costs and increase customer satisfaction."
      ],
      "metadata": {
        "id": "zLkt1OlHPuAm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Case Study Context**\n",
        "\n",
        "This case study focuses on a mid-sized retail company facing challenges with inventory management and sustainability. The goal is to apply inventory optimization and sustainability analysis strategies to improve the companyâ€™s operational efficiency.\n",
        "\n",
        "The company has a variety of products with different life cycles and market demands, making inventory management complex and costly. Through detailed analysis of sales, procurement, and inventory data, the company seeks to minimize waste, optimize inventory levels, and adopt a more sustainable and profitable approach to managing its resources.\n",
        "\n",
        "**Goals**\n",
        "- Minimize waste by optimizing inventory levels. Identify the most efficient purchasing strategies based on sales, procurement, and inventory data. Evaluate product sales performance to formulate a sustainable inventory management approach. Data Preprocessing.\n",
        "- Consolidate all CSV files into a unified master dataset. Check for missing or erroneous entries. Standardize date formats for consistent time series analysis. Inventory Analysis.\n",
        "- Assess inventory status at the beginning and end of the year using BegInvFINAL12312016.csv and EndInvFINAL12312016.csv. Identify products with the highest and lowest inventory presence. Sales Analysis.\n",
        "- Examine SalesFINAL12312016.csv to identify top sellers and slow-selling products. Analyze sales trends over time, considering variables such as sales quantity, sales price, and date. Purchasing Analysis.\n",
        "- Evaluate procurement activities using PurchasesFINAL12312016.csv and InvoicePurchases12312016.csv. Investigate purchase volumes from different suppliers, procurement costs, and supply chain processes. Calculate Optimal Inventory Level.\n",
        "- Determine the optimal inventory level for each product, leveraging sales, procurement, and inventory data. Propose inventory levels adapted to product sales velocity and supply times. Conclusion: The insights gained from these analyses will provide recommendations for managing inventory more efficiently and sustainably, aiming to reduce costs and prevent excess inventory and waste."
      ],
      "metadata": {
        "id": "CHG7_e-9Wo4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing all the necessary libraries:\n",
        "import gdown\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "TpQOMg_gXsnI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the datasets:\n",
        "!gdown 10hu07auuv2wijftTvUfgWAAf3YjXddC3"
      ],
      "metadata": {
        "id": "a0bqdTU8QjHy",
        "outputId": "639eb17d-53fb-4616-83a5-8d89ac8d578f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to retrieve file url:\n",
            "\n",
            "\tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses.\n",
            "\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\thttps://drive.google.com/uc?id=10hu07auuv2wijftTvUfgWAAf3YjXddC3\n",
            "\n",
            "but Gdown can't. Please check connections and permissions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing all the necessary datasets:\n",
        "purchase_prices = pd.read_csv('2017PurchasePricesDec.csv')\n",
        "beg_inv = pd.read_csv('BegInvFINAL12312016.csv')\n",
        "end_inv = pd.read_csv('EndInvFINAL12312016.csv')\n",
        "invoice_purchases = pd.read_csv('InvoicePurchases12312016.csv')\n",
        "purchases = pd.read_csv('PurchasesFINAL12312016.csv')\n",
        "sales = pd.read_csv('SalesFINAL12312016.csv')"
      ],
      "metadata": {
        "id": "sXd3m2JhX9OD",
        "outputId": "46dc8bc7-296f-4f1c-9ff3-9a10dc8720ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '2017PurchasePricesDec.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-0eebdc9a350f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Importing all the necessary datasets:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpurchase_prices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2017PurchasePricesDec.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbeg_inv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BegInvFINAL12312016.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mend_inv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'EndInvFINAL12312016.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minvoice_purchases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'InvoicePurchases12312016.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '2017PurchasePricesDec.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Groupting by Brand and Description and summarizing the inventory at the start of the year:\n",
        "start_summary = beg_inv.groupby(['Brand', 'Description'])['onHand'].sum().sort_values(ascending=False)\n",
        "\n",
        "# Groupting by Brand and Description and summarizing the inventory at the end of the year:\n",
        "end_summary = end_inv.groupby(['Brand', 'Description'])['onHand'].sum().sort_values(ascending=False)\n",
        "\n",
        "# Identifying the most popular productus at the start and end of the year:\n",
        "top_5_start = start_summary.head(5)\n",
        "top_5_end = end_summary.head(5)\n",
        "\n",
        "# Identifying the least popular productus at the start and end of the year:\n",
        "bottom_5_start = start_summary.tail(5)\n",
        "bottom_5_end = end_summary.tail(5)\n",
        "\n",
        "# Dislaying the results:\n",
        "print('Top 5 products at the start of the year:\\n', top_5_start)\n",
        "print('\\nTop 5 products at the end of the year:\\n', top_5_end)\n",
        "print('\\nBottom 5 products at the start of the year:\\n', bottom_5_start)\n",
        "print('\\nBottom 5 products at the end of the year:\\n', bottom_5_end)"
      ],
      "metadata": {
        "id": "pVvzaljfYBVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the most sold produlcts:\n",
        "most_sold = sales.groupby(['Brand', 'Description']).agg({'SalesQuantity': 'sum'}).sort_values(by='SalesQuantity', ascending=False).head(10)\n",
        "print(f'10 most sold products:\\n{most_sold}\\n')\n",
        "\n",
        "# Finding the least sold produlcts:\n",
        "least_sold = sales.groupby(['Brand', 'Description']).agg({'SalesQuantity': 'sum'}).sort_values(by='SalesQuantity', ascending=True).head(10)\n",
        "print(f'10 least sold products:\\n{least_sold}\\n')"
      ],
      "metadata": {
        "id": "71jk5v8fZcdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "purchases['PODate'] = pd.to_datetime(purchases['PODate'], errors='coerce')\n",
        "purchases['ReceivingDate'] = pd.to_datetime(purchases['ReceivingDate'], errors='coerce')\n",
        "purchases['SupplyDuration'] = (purchases['ReceivingDate'] - purchases['PODate']).dt.days\n",
        "average_supply_duration = purchases['SupplyDuration'].mean()\n",
        "print('Average duration of supply (in days):', average_supply_duration)"
      ],
      "metadata": {
        "id": "deIZfsQuaFSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "purchases['InvoiceDate'] = pd.to_datetime(purchases['InvoiceDate'])\n",
        "purchases['PayDate'] = pd.to_datetime(purchases['PayDate'])\n",
        "purchases['PaymentDuration'] = (purchases['PayDate'] - purchases['InvoiceDate']).dt.days\n",
        "average_payment_duration = purchases['PaymentDuration'].mean()\n",
        "print('Average payment duration (in days):', average_payment_duration)"
      ],
      "metadata": {
        "id": "_a3HVefsaMcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the histogram styles:\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "# Histogram for the supply duration:\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(purchases['SupplyDuration'], kde=True, bins=30, color='coral')\n",
        "plt.title('Supply Duration')\n",
        "plt.xlabel('Supply Duration (in days)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5ABFs_vjawNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogram for the payment duration:\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(purchases['PaymentDuration'], kde=True, bins=30, color='teal')\n",
        "plt.title('Payment Duration')\n",
        "plt.xlabel('Payment Duration (in days)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h36ZlKOEbDYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determining the sales period:\n",
        "sales['SalesDate'] = pd.to_datetime(sales['SalesDate'])\n",
        "start_date = sales['SalesDate'].min()\n",
        "end_date = sales['SalesDate'].max()\n",
        "total_days = (end_date - start_date).days\n",
        "total_days"
      ],
      "metadata": {
        "id": "7Ib33M3Cf1E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the sale speed for each product:\n",
        "sales_velocity = sales.groupby(['Brand', 'Description']).agg(Total_Sales=('SalesQuantity', 'sum')).reset_index()\n",
        "sales_velocity['Sales_Per_Day'] = sales_velocity['Total_Sales'] / total_days\n",
        "sales_velocity['Sales_Per_Day']"
      ],
      "metadata": {
        "id": "b2a51OtWf6uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the delivery time:\n",
        "purchases.loc[:, 'Lead_Time'] = (purchases['ReceivingDate'] - purchases['PODate']).dt.days\n",
        "lead_times = purchases.groupby(['Brand', 'Description']).agg(Avg_Lead_Time=('Lead_Time', 'mean')).reset_index()\n",
        "lead_times"
      ],
      "metadata": {
        "id": "CXsZ3LSSgHzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging Sales and Purchase Data:\n",
        "merged_data = pd.merge(sales_velocity, lead_times, on=['Brand', 'Description'], how='left')\n",
        "merged_data"
      ],
      "metadata": {
        "id": "Mp2cirWegQ5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Safety Stock using maximum sales for each product:\n",
        "max_vendas = sales.groupby(['Brand', 'Description']).agg(Max_Vendas_Diarias=('SalesQuantity', 'max')).reset_index()\n",
        "merged_data = pd.merge(merged_data, max_vendas, on=['Brand', 'Description'], how='left')\n",
        "merged_data"
      ],
      "metadata": {
        "id": "TW5sGx4bgg9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the Optimal Stock Level:\n",
        "merged_data['Optimal_Stock_Level'] = merged_data['Sales_Per_Day'] * merged_data['Avg_Lead_Time']\n",
        "merged_data['Optimal_Stock_Level']"
      ],
      "metadata": {
        "id": "5DzUaPolgpgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Safety Stock:\n",
        "max_sales = sales.groupby(['Brand', 'Description']).agg(Max_Daily_Sales=('SalesQuantity', 'max')).reset_index()\n",
        "merged_data = pd.merge(merged_data, max_sales, on=['Brand', 'Description'], how='left')\n",
        "merged_data['Safety_Stock'] = merged_data['Max_Daily_Sales'] - merged_data['Sales_Per_Day']\n",
        "merged_data['Recommended_Stock_Level'] = merged_data['Optimal_Stock_Level'] + merged_data['Safety_Stock']\n",
        "merged_data['Recommended_Stock_Level']"
      ],
      "metadata": {
        "id": "UbDlzo3YhW0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sorting data by Recommended Stock Level for better visualization:\n",
        "ordered_data = merged_data.sort_values(by='Recommended_Stock_Level', ascending=False)\n",
        "\n",
        "# Plotando\n",
        "plt.figure(figsize=(15, 10))\n",
        "sns.barplot(x='Recommended_Stock_Level', y='Description', data=ordered_data.head(20), palette='viridis')  # mostrando os 20 produtos principais para melhor visualizaÃ§Ã£o\n",
        "plt.xlabel('Recommended Stock Level')\n",
        "plt.ylabel('Product Description')\n",
        "plt.title('Recommended Stock Levels for Top 20 Products')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CAIO0Y33hjTp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}